{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13daa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "from IPython.display import Audio, HTML, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from chord_progressions.io.audio import combine_buffers\n",
    "from chord_progressions.utils import is_circular_match\n",
    "from chord_progressions.type_templates import get_template_from_template_str\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rhythmic_relationships.data import PartPairDataset\n",
    "\n",
    "from notebook_utils import (\n",
    "    get_chroma_vocab,\n",
    "    get_audio_el,\n",
    "    mk_voiced_chroma_buffer,\n",
    "    get_voiced_hits_from_chroma,\n",
    ")\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c495cb",
   "metadata": {},
   "source": [
    "## Load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_VOCAB = get_chroma_vocab()\n",
    "inv_vocab = {v: k for k, v in CHROMA_VOCAB.items()}\n",
    "stoi = {s: i for i, s in enumerate(inv_vocab)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "itot = {ix: i for ix, i in enumerate(list(CHROMA_VOCAB))}\n",
    "ttoi = {v: k for k, v in itot.items()}\n",
    "\n",
    "vocab_size = len(CHROMA_VOCAB)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "def get_type_from_template(template):\n",
    "    for chord_type in list(CHROMA_VOCAB):\n",
    "        if is_circular_match(\n",
    "            template,\n",
    "            get_template_from_template_str(CHROMA_VOCAB[chord_type]),\n",
    "        ):\n",
    "            return chord_type\n",
    "    return None\n",
    "\n",
    "def pclist_to_i(pclist):\n",
    "    \"\"\"Gets a chord type index from a pitch class list\n",
    "    e.g. pclist_to_i([1,0,0,0,0,0,0,0,0,0,0,0]) -> 1\n",
    "    \"\"\"\n",
    "    chord_type = get_type_from_template(pclist)\n",
    "    if not chord_type:\n",
    "        chord_type = \"oov\"\n",
    "    template_str = CHROMA_VOCAB[chord_type]\n",
    "    return stoi[template_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f401c73",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_4res\",\n",
    "    \"part_1\": 'Guitar',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"chroma\",\n",
    "    \"repr_2\": \"chroma\",\n",
    "}\n",
    "\n",
    "data = PartPairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1, shuffle=True)\n",
    "\n",
    "# How many chords types do we use to predict the next one\n",
    "context_len = 8\n",
    "\n",
    "# Hold an even amount of x and y; hold an extra step of y if the context length is odd\n",
    "n_ys = context_len // 2 if context_len % 2 == 0 else context_len // 2 + 1\n",
    "n_xs = context_len // 2\n",
    "\n",
    "silence_i = stoi[CHROMA_VOCAB['silence']]\n",
    "\n",
    "prev_xs = []\n",
    "X, Y = [], []\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the chromas\n",
    "    x = (x > 1).to(torch.int32)[0]\n",
    "    y = (y > 1).to(torch.int32)[0]\n",
    "\n",
    "    # fill context with silence\n",
    "    context = [silence_i] * context_len\n",
    "\n",
    "    for xrow, yrow in zip(x, y):\n",
    "        ixx = pclist_to_i(xrow.tolist())\n",
    "        ixy = pclist_to_i(yrow.tolist())\n",
    "        # print(','.join(itot[i] for i in context), '-->', itot[ixy])\n",
    "\n",
    "        X.append(context)\n",
    "        prev_xs.append(ixx)\n",
    "        Y.append(ixy)\n",
    "\n",
    "        context = Y[-n_ys:] + prev_xs[-n_xs:]\n",
    "        while len(context) < context_len:\n",
    "            context = [silence_i] + context\n",
    "\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n_examples = X.nelement()\n",
    "print(f'{n_examples=}')\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64753a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(X[:10], Y[:10]):\n",
    "    print(','.join([itot[ix.item()] for ix in x]), '-->', itot[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36671fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            if x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdim=True)  # batch mean\n",
    "            xvar = x.var(dim, keepdim=True)  # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bengio et al has a vocab size of 17k and they embed them in a 30-dimensional space\n",
    "# Our vocab size is much smaller, so we can use a much smaller embedding space\n",
    "\n",
    "n_embed = 36  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 500  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embed),\n",
    "    FlattenConsecutive(2), Linear(n_embed * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f'num total params: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071428d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at a batch of just 4 examples\n",
    "# ix = torch.randint(0, X.shape[0], (4,))\n",
    "# Xb, Yb = X[ix], Y[ix]\n",
    "# logits = model(Xb)\n",
    "# print(Xb.shape)\n",
    "# print(Xb)\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd43f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 50000\n",
    "batch_size = 128\n",
    "lossi = []\n",
    "ud = [] # update:data ratio\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "    Xb, Yb = X[ix], Y[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 40000 else 0.01  # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append(\n",
    "            [((lr * p.grad).std() / p.data.std()).log10().item() for p in parameters]\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b399cb",
   "metadata": {},
   "source": [
    "## performance log\n",
    "- original (3 character context, 200 hidden neurons, 172028 params): 1.0094\n",
    "- context 3->8 (182028 params): 0.5128\n",
    "- flat -> hierarchical (169209 params): 0.3639\n",
    "- scale up: n_embed = 36, n_hidden = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43989212",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 3  # seconds\n",
    "\n",
    "# Get an examples from the dataset for reference\n",
    "x, y = next(iter(loader))\n",
    "\n",
    "# Binarize the chromas\n",
    "x = (x > 1).to(torch.int32)[0]\n",
    "y = (y > 1).to(torch.int32)[0]\n",
    "\n",
    "x_t = []\n",
    "x_chord_types = []\n",
    "\n",
    "y_t = []\n",
    "y_chord_types = []\n",
    "for xrow, yrow in zip(x, y):\n",
    "    ixx = pclist_to_i(xrow.tolist())\n",
    "    ixy = pclist_to_i(yrow.tolist())\n",
    "    \n",
    "    x_t.append(ixx)\n",
    "    y_t.append(ixy)\n",
    "\n",
    "    x_chord_types.append(itot[ixx])\n",
    "    y_chord_types.append(itot[ixy])\n",
    "\n",
    "x_t = [ttoi[''.join(map(str, i))] for i in x_chord_types]\n",
    "x_chroma = np.array([list(map(int, CHROMA_VOCAB[i])) for i in x_chord_types])\n",
    "x_voiced_hits = get_voiced_hits_from_chroma(x_chroma)\n",
    "x_buff = mk_voiced_chroma_buffer(x_voiced_hits, duration=duration, n_overtones=1)\n",
    "x_hits = (x_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "\n",
    "y_chroma = np.array([list(map(int, list(itos[i]))) for i in y_t])\n",
    "y_hits = (y_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "y_voiced_hits = get_voiced_hits_from_chroma(y_chroma)\n",
    "y_buff = mk_voiced_chroma_buffer(y_voiced_hits, duration=duration, n_overtones=4)\n",
    "\n",
    "buffs = collections.defaultdict(list)\n",
    "buffs[\"\".join(map(str, y_hits))].append([y_buff, combine_buffers([x_buff, y_buff])])\n",
    "\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{\"\".join(map(str, x_hits))}</br>{get_audio_el(x_buff)}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from learned distribution'\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d97a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# disable gradient tracking\n",
    "@torch.no_grad()\n",
    "def disable_grads():\n",
    "    return\n",
    "\n",
    "# Set layers to eval mode\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# sample from the model\n",
    "n_samples = 10\n",
    "\n",
    "input_chord_types = x_chord_types\n",
    "\n",
    "in_t = [ttoi[''.join(map(str, i))] for i in input_chord_types]\n",
    "\n",
    "in_chroma = np.array([list(map(int, CHROMA_VOCAB[i])) for i in input_chord_types])\n",
    "in_voiced_hits = get_voiced_hits_from_chroma(in_chroma)\n",
    "in_buff = mk_voiced_chroma_buffer(in_voiced_hits, duration=duration, n_overtones=1)\n",
    "in_hits = (in_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "\n",
    "buffs = collections.defaultdict(list)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    out = []\n",
    "\n",
    "    context = [silence_i] * (context_len - 1) + [in_t[0]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tix, t in enumerate(in_t):\n",
    "            # Forward pass the neural net\n",
    "            logits = model(torch.tensor([context]))\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            out.append(ix)\n",
    "\n",
    "            # Shift the context window and track the samples\n",
    "            context = out[-n_ys:] + in_t[-n_xs:]\n",
    "            while len(context) < context_len:\n",
    "                context = [silence_i] + context\n",
    "\n",
    "    out_chroma = np.array([list(map(int, list(itos[i]))) for i in out])\n",
    "    out_hits = (out_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    out_voiced_hits = get_voiced_hits_from_chroma(out_chroma)\n",
    "    out_buff = mk_voiced_chroma_buffer(out_voiced_hits, duration=duration, n_overtones=4)\n",
    "    buffs[\"\".join(map(str, out_hits))].append([out_buff, combine_buffers([in_buff, out_buff])])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{\"\".join(map(str, in_hits))}</br>{get_audio_el(in_buff)}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from model'\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a506ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
