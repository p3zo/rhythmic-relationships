{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142c2946",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e912277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import wave\n",
    "from base64 import b64encode\n",
    "from io import BytesIO\n",
    "from IPython.display import Audio, HTML, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from chord_progressions.chord import get_note_from_midi_num\n",
    "from chord_progressions.io.audio import mk_arpeggiated_chord_buffer, combine_buffers, mk_chord_buffer, mk_sin\n",
    "from chord_progressions.solver import select_voicing\n",
    "from chord_progressions.type_templates import get_template_from_template_str, TYPE_TEMPLATES\n",
    "from chord_progressions.utils import is_circular_match\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rhythmic_complements.data import PairDataset\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def get_type_from_template(template):\n",
    "    for chord_type in list(TYPE_TEMPLATES):\n",
    "        if is_circular_match(\n",
    "            template,\n",
    "            get_template_from_template_str(TYPE_TEMPLATES[chord_type]),\n",
    "        ):\n",
    "            return chord_type\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def mk_wav(arr):\n",
    "    \"\"\"Transform a numpy array to a PCM bytestring\n",
    "    Adapted from https://github.com/ipython/ipython/blob/main/IPython/lib/display.py#L146\n",
    "    \"\"\"\n",
    "    scaled = arr * 32767\n",
    "    scaled = scaled.astype(\"<h\").tobytes()\n",
    "\n",
    "    fp = BytesIO()\n",
    "    waveobj = wave.open(fp,mode='wb')\n",
    "    waveobj.setnchannels(1)\n",
    "    waveobj.setframerate(SAMPLE_RATE)\n",
    "    waveobj.setsampwidth(2)\n",
    "    waveobj.setcomptype('NONE','NONE')\n",
    "    waveobj.writeframes(scaled)\n",
    "    val = fp.getvalue()\n",
    "    waveobj.close()\n",
    "\n",
    "    return val\n",
    "\n",
    "def get_audio_el(audio):\n",
    "    wav = mk_wav(audio)\n",
    "    b64 = b64encode(wav).decode('ascii')\n",
    "    return f'<audio controls=\"controls\"><source src=\"data:audio/wav;base64,{b64}\" type=\"audio/wav\"/></audio>'\n",
    "\n",
    "def get_voiced_hits_from_chroma(chroma):\n",
    "    voiced = []\n",
    "    for c in chroma:\n",
    "        voiced.append(select_voicing(c.tolist(), note_range_low=60, note_range_high=72))\n",
    "    return voiced\n",
    "\n",
    "def mk_voiced_chroma_buffer(voiced_hits, duration, n_overtones):\n",
    "    bufs = []\n",
    "\n",
    "    for hit in voiced_hits:\n",
    "        pos_dur = duration / len(voiced_hits)\n",
    "\n",
    "        buf = mk_sin(0, pos_dur, 0).squeeze()\n",
    "        if hit:\n",
    "            chord = [get_note_from_midi_num(m) for m in hit]\n",
    "            buf = mk_chord_buffer(chord, pos_dur, n_overtones)\n",
    "        bufs.append(buf)\n",
    "\n",
    "    return np.concatenate(bufs, axis=0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d13b7",
   "metadata": {},
   "source": [
    "# Hits\n",
    "## Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f040e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_24res\",\n",
    "    \"part_1\": 'Bass',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"hits\",\n",
    "    \"repr_2\": \"hits\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "n = 2\n",
    "N = torch.zeros((n, n), dtype=torch.int32)\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    x = x.to(torch.int32)\n",
    "    y = y.to(torch.int32)\n",
    "    for h1, h2 in zip(x.flatten(), y.flatten()):\n",
    "        ix1 = h1.item()\n",
    "        ix2 = h2.item()\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(P, cmap='Blues')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        chstr = str(i) + str(j)\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, round(P[i, j].item(), 3), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420673a4",
   "metadata": {},
   "source": [
    "## Sample hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 3\n",
    "audio_duration = 3  # seconds\n",
    "n_overtones = 0\n",
    "\n",
    "input_hits = [1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
    "inbuff = mk_arpeggiated_chord_buffer([\"C4\"], audio_duration, [input_hits], n_overtones)\n",
    "inhitstr = \"\".join(map(str, input_hits))\n",
    "\n",
    "buffs = {}\n",
    "rand_buffs = {}\n",
    "for i in range(n_samples):\n",
    "    out = []\n",
    "    rand_out = []\n",
    "    for el in input_hits:\n",
    "        p = P[el]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(ix)\n",
    "\n",
    "        rand_p = torch.ones(n) / n  # uniform distribution\n",
    "        rand_ix = torch.multinomial(\n",
    "            rand_p, num_samples=1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        rand_out.append(rand_ix)\n",
    "\n",
    "    outbuff = mk_arpeggiated_chord_buffer([\"G4\"], audio_duration, [out], n_overtones)\n",
    "    buffs[\"\".join(map(str, out))] = combine_buffers([inbuff, outbuff])\n",
    "\n",
    "    rand_hbuff = mk_arpeggiated_chord_buffer(\n",
    "        [\"G4\"], audio_duration, [rand_out], n_overtones\n",
    "    )\n",
    "    rand_buffs[\"\".join(map(str, rand_out))] = combine_buffers([inbuff, rand_hbuff])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{get_audio_el(inbuff)}{inhitstr}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from bigram distribution'\n",
    "    + \"\".join([f\"</br>{get_audio_el(v)} {k}\" for k, v in buffs.items()])\n",
    "    + \"</br></br>Samples from uniform distribution\"\n",
    "    + \"\".join([f\"</br>{get_audio_el(v)} {k}\" for k, v in rand_buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3c742",
   "metadata": {},
   "source": [
    "# Chroma\n",
    "\n",
    "## Creating chroma bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071006b",
   "metadata": {},
   "source": [
    "Binary chromas are (N, 12) matrices of pitch class activations, where N is the number of timesteps and binary values show the absence or presence of the pitch class.\n",
    "\n",
    "We could use binary pitch class strings of length 12 (e.g. `100100100100`) as tokens. That would give us a vocab size of 2**12 = 4096 (e.g. `perms = [''.join(map(str, i)) for i in itertools.product([0, 1], repeat=12)]` ). With the 36752 pairs in `babyslakh_20` that would give us a data : vocab ratio of ~9. Makemore has a 8500x data : vocab ratio (228146 pairs : vocab 27). To get a similar 8500x ratio we need ~35M pairs. Assuming the `babyslakh_20` average of ~1.8k pairs per file, we'd need ~19k files. `lmd_clean` is 17k files which is close, but it takes a long time to process and work with.\n",
    "\n",
    "We opt to instead shrink the vocab size by labeling the pitch class strings with [chord types](https://github.com/p3zo/chord-progressions/blob/main/chord_progressions/type_templates.py). That gives a vocab size of 193 (191 templates + 1 silence token + 1 out-of-vocab token). A vocab size of 193 wants ~1.7M pairs for the same ratio as makemore, which should be around 1k files. So we use `lmd_clean_1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e31850",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_24res\",\n",
    "    \"part_1\": 'Guitar',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"chroma\",\n",
    "    \"repr_2\": \"chroma\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "TYPE_TEMPLATES.update({'silence': '000000000000'})\n",
    "n = len(TYPE_TEMPLATES)\n",
    "print(f'Vocab size: {n}')\n",
    "\n",
    "N = torch.zeros((n, n), dtype=torch.int32)\n",
    "\n",
    "inv_type_templates = {v: k for k, v in TYPE_TEMPLATES.items()}\n",
    "stoi = {s:i for i,s in enumerate(inv_type_templates)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    x = x.to(torch.int32)\n",
    "    y = y.to(torch.int32)\n",
    "    for h1, h2 in zip(x, y):\n",
    "        for xrow, yrow in zip(h1, h2):\n",
    "            xtype = get_type_from_template(xrow.tolist())\n",
    "            ytype =  get_type_from_template(yrow.tolist())\n",
    "            x_template_str = TYPE_TEMPLATES[xtype]\n",
    "            y_template_str = TYPE_TEMPLATES[ytype]\n",
    "            ixx = stoi[x_template_str]\n",
    "            ixy = stoi[y_template_str]\n",
    "            N[ixx, ixy] += 1\n",
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "print(N.float().min(), N.float().max())\n",
    "print(P.min(), P.max())\n",
    "print(N.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4eef9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = f\"{dataset_config['part_1']}, {dataset_config['part_2']}\"\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(N, cmap='Blues');\n",
    "plt.title(title)\n",
    "plt.xticks(range(n), labels=list(TYPE_TEMPLATES), rotation=90, fontsize=5);\n",
    "plt.yticks(range(n), labels=list(TYPE_TEMPLATES), fontsize=5);\n",
    "for i in tqdm(range(n)):\n",
    "    for j in range(n):\n",
    "#         chstr = '_'.join([inv_type_templates[itos[i]], inv_type_templates[itos[j]]])\n",
    "        chstr = '|'.join([str(i), str(j)])\n",
    "        val = N[i, j].item()\n",
    "        if val > 1:\n",
    "            plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray', fontsize=6)\n",
    "            plt.text(j, i, round(val, 2), ha=\"center\", va=\"top\", color='gray', fontsize=5)\n",
    "\n",
    "plt.savefig(f'chroma_bigrams/{title}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80a8ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataset_config['part_1'], dataset_config['part_2'])\n",
    "for ix, i in enumerate(TYPE_TEMPLATES):\n",
    "    v = N[ix].sum().item()\n",
    "    if v > 0:\n",
    "        print(v, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f11d4",
   "metadata": {},
   "source": [
    "## Sampling chroma\n",
    "\n",
    "Using chord types as our vocabulary has the added benefit of being able to use can use `chord_progressions.solver` to find voicings for templates that we sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173a607",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 3\n",
    "duration = 3  # seconds\n",
    "\n",
    "input_chord_types = [\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"unison\",\n",
    "    \"minor chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "]\n",
    "in_chroma = np.array([list(map(int, TYPE_TEMPLATES[i])) for i in input_chord_types])\n",
    "in_voiced_hits = get_voiced_hits_from_chroma(in_chroma)\n",
    "in_buff = mk_voiced_chroma_buffer(in_voiced_hits, duration=duration, n_overtones=1)\n",
    "in_hits = (in_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "\n",
    "buffs = collections.defaultdict(list)\n",
    "rand_buffs = collections.defaultdict(list)\n",
    "for i in range(n_samples):\n",
    "    rand_chroma = []\n",
    "    out_chroma = []\n",
    "    for t in in_chroma:\n",
    "        tix = stoi[TYPE_TEMPLATES[get_type_from_template(t)]]\n",
    "        p = P[tix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        pchromavec = list(map(int, itos[ix]))\n",
    "        out_chroma.append(pchromavec)\n",
    "\n",
    "        rand_p = torch.ones(n) / n  # uniform distribution\n",
    "        rand_ix = torch.multinomial(rand_p, num_samples=1, replacement=True, generator=g).item()\n",
    "        # For now, map out-of-vocabulary token to silence\n",
    "        if rand_ix == 0:\n",
    "            rand_ix = 192\n",
    "        rand_pchromavec = list(map(int, itos[rand_ix]))\n",
    "        rand_chroma.append(rand_pchromavec)\n",
    "\n",
    "    out_chroma = np.array(out_chroma)\n",
    "    out_hits = (out_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    out_voiced_hits = get_voiced_hits_from_chroma(out_chroma)\n",
    "    out_buff = mk_voiced_chroma_buffer(out_voiced_hits, duration=duration, n_overtones=4)\n",
    "    buffs[\"\".join(map(str, out_hits))].append([out_buff, combine_buffers([in_buff, out_buff])])\n",
    "\n",
    "    rand_chroma = np.array(rand_chroma)\n",
    "    rand_hits = (rand_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    rand_voiced_hits = get_voiced_hits_from_chroma(rand_chroma)\n",
    "    rand_buff = mk_voiced_chroma_buffer(rand_voiced_hits, duration=duration, n_overtones=4)\n",
    "    rand_buffs[\"\".join(map(str, rand_hits))].append([rand_buff, combine_buffers([in_buff, rand_buff])])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{\"\".join(map(str, in_hits))}</br>{get_audio_el(in_buff)}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from bigram distribution'\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in buffs.items()])\n",
    "    + \"</br></br>Samples from uniform distribution\"\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in rand_buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa2c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
