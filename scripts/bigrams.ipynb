{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e912277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f040e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rhythmic_complements.data import PairDataset\n",
    "\n",
    "part_1 = 'Bass'\n",
    "part_2 = 'Piano'\n",
    "\n",
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_24res\",\n",
    "    \"part_1\": part_1,\n",
    "    \"part_2\": part_2,\n",
    "    \"repr_1\": \"hits\",\n",
    "    \"repr_2\": \"hits\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "n = 2\n",
    "N = torch.zeros((n, n), dtype=torch.int32)\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    x = x.to(torch.int32)\n",
    "    y = y.to(torch.int32)\n",
    "    for h1, h2 in zip(x.flatten(), y.flatten()):\n",
    "        ix1 = h1.item()\n",
    "        ix2 = h2.item()\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(P, cmap='Blues')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        chstr = str(i) + str(j)\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, round(P[i, j].item(), 3), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from base64 import b64encode\n",
    "from io import BytesIO\n",
    "from IPython.display import Audio, HTML, display\n",
    "\n",
    "import numpy as np\n",
    "from chord_progressions.io.audio import mk_arpeggiated_chord_buffer, combine_buffers\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "\n",
    "def mk_wav(arr):\n",
    "    \"\"\"Transform a numpy array to a PCM bytestring\n",
    "    Adapted from https://github.com/ipython/ipython/blob/main/IPython/lib/display.py#L146\n",
    "    \"\"\"\n",
    "    scaled = arr * 32767\n",
    "    scaled = scaled.astype(\"<h\").tobytes()\n",
    "\n",
    "    fp = BytesIO()\n",
    "    waveobj = wave.open(fp,mode='wb')\n",
    "    waveobj.setnchannels(1)\n",
    "    waveobj.setframerate(SAMPLE_RATE)\n",
    "    waveobj.setsampwidth(2)\n",
    "    waveobj.setcomptype('NONE','NONE')\n",
    "    waveobj.writeframes(scaled)\n",
    "    val = fp.getvalue()\n",
    "    waveobj.close()\n",
    "\n",
    "    return val\n",
    "\n",
    "def get_audio_el(audio):\n",
    "    wav = mk_wav(audio)\n",
    "    b64 = b64encode(wav).decode('ascii')\n",
    "    return f'<audio controls=\"controls\"><source src=\"data:audio/wav;base64,{b64}\" type=\"audio/wav\"/></audio>'\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 5\n",
    "pattern_len = len(next(iter(loader))[0][0])\n",
    "\n",
    "audio_duration = 3 # seconds\n",
    "n_overtones = 0\n",
    "\n",
    "input_hits = [1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
    "inbuff = mk_arpeggiated_chord_buffer(['C4'], audio_duration, [input_hits], n_overtones)\n",
    "inhitstr = ''.join(map(str, input_hits))\n",
    "\n",
    "buffs = {}\n",
    "for i in range(n_samples):\n",
    "    out = []\n",
    "    for el in input_hits:\n",
    "        p = P[el]\n",
    "#         p = torch.ones(2) / 2. # uniform\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(ix)\n",
    "\n",
    "    hbuff = mk_arpeggiated_chord_buffer(['G4'], audio_duration, [out], n_overtones)\n",
    "    buffs[''.join(map(str, out))] = combine_buffers([inbuff, hbuff])\n",
    "    \n",
    "html = ''.join([f\"<p>{get_audio_el(v)} <p>{inhitstr} {part_1} input</br>{k} {part_2} prediction</p></p>\" for k,v in buffs.items()])\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a3345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
