{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142c2946",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e912277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import wave\n",
    "from base64 import b64encode\n",
    "from io import BytesIO\n",
    "from IPython.display import Audio, HTML, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from chord_progressions.chord import get_note_from_midi_num\n",
    "from chord_progressions.io.audio import mk_arpeggiated_chord_buffer, combine_buffers, mk_chord_buffer, mk_sin\n",
    "from chord_progressions.solver import select_voicing\n",
    "from chord_progressions.type_templates import get_template_from_template_str, TYPE_TEMPLATES\n",
    "from chord_progressions.utils import is_circular_match\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rhythmic_relationships.data import PairDataset\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def get_type_from_template(template):\n",
    "    for chord_type in list(TYPE_TEMPLATES):\n",
    "        if is_circular_match(\n",
    "            template,\n",
    "            get_template_from_template_str(TYPE_TEMPLATES[chord_type]),\n",
    "        ):\n",
    "            return chord_type\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def mk_wav(arr):\n",
    "    \"\"\"Transform a numpy array to a PCM bytestring\n",
    "    Adapted from https://github.com/ipython/ipython/blob/main/IPython/lib/display.py#L146\n",
    "    \"\"\"\n",
    "    scaled = arr * 32767\n",
    "    scaled = scaled.astype(\"<h\").tobytes()\n",
    "\n",
    "    fp = BytesIO()\n",
    "    waveobj = wave.open(fp,mode='wb')\n",
    "    waveobj.setnchannels(1)\n",
    "    waveobj.setframerate(SAMPLE_RATE)\n",
    "    waveobj.setsampwidth(2)\n",
    "    waveobj.setcomptype('NONE','NONE')\n",
    "    waveobj.writeframes(scaled)\n",
    "    val = fp.getvalue()\n",
    "    waveobj.close()\n",
    "\n",
    "    return val\n",
    "\n",
    "def get_audio_el(audio):\n",
    "    wav = mk_wav(audio)\n",
    "    b64 = b64encode(wav).decode('ascii')\n",
    "    return f'<audio controls=\"controls\"><source src=\"data:audio/wav;base64,{b64}\" type=\"audio/wav\"/></audio>'\n",
    "\n",
    "def get_voiced_hits_from_chroma(chroma):\n",
    "    voiced = []\n",
    "    for c in chroma:\n",
    "        voiced.append(select_voicing(c.tolist(), note_range_low=60, note_range_high=72))\n",
    "    return voiced\n",
    "\n",
    "def mk_voiced_chroma_buffer(voiced_hits, duration, n_overtones):\n",
    "    bufs = []\n",
    "\n",
    "    for hit in voiced_hits:\n",
    "        pos_dur = duration / len(voiced_hits)\n",
    "\n",
    "        buf = mk_sin(0, pos_dur, 0).squeeze()\n",
    "        if hit:\n",
    "            chord = [get_note_from_midi_num(m) for m in hit]\n",
    "            buf = mk_chord_buffer(chord, pos_dur, n_overtones)\n",
    "        bufs.append(buf)\n",
    "\n",
    "    return np.concatenate(bufs, axis=0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d13b7",
   "metadata": {},
   "source": [
    "# Hits\n",
    "## Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f040e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_4res\",\n",
    "    \"part_1\": 'Bass',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"hits\",\n",
    "    \"repr_2\": \"hits\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "n = 2\n",
    "N = torch.zeros((n, n), dtype=torch.int32)\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the hits\n",
    "    x = (x > 0).to(torch.int32)\n",
    "    y = (y > 0).to(torch.int32)\n",
    "    for h1, h2 in zip(x.flatten(), y.flatten()):\n",
    "        ix1 = h1.item()\n",
    "        ix2 = h2.item()\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "# Adding a count of 1 to ensure there are no 0s in our probability matrix P\n",
    "# This is called model smoothing\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(P, cmap='Blues')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        chstr = str(i) + str(j)\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, round(P[i, j].item(), 3), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420673a4",
   "metadata": {},
   "source": [
    "## Sample hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 3\n",
    "audio_duration = 3  # seconds\n",
    "n_overtones = 0\n",
    "\n",
    "input_hits = [1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
    "inbuff = mk_arpeggiated_chord_buffer([\"C4\"], audio_duration, [input_hits], n_overtones)\n",
    "inhitstr = \"\".join(map(str, input_hits))\n",
    "\n",
    "buffs = {}\n",
    "rand_buffs = {}\n",
    "for i in range(n_samples):\n",
    "    out = []\n",
    "    rand_out = []\n",
    "    for el in input_hits:\n",
    "        p = P[el]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(ix)\n",
    "\n",
    "        rand_p = torch.ones(n) / n  # uniform distribution\n",
    "        rand_ix = torch.multinomial(\n",
    "            rand_p, num_samples=1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        rand_out.append(rand_ix)\n",
    "\n",
    "    outbuff = mk_arpeggiated_chord_buffer([\"G4\"], audio_duration, [out], n_overtones)\n",
    "    buffs[\"\".join(map(str, out))] = combine_buffers([inbuff, outbuff])\n",
    "\n",
    "    rand_hbuff = mk_arpeggiated_chord_buffer(\n",
    "        [\"G4\"], audio_duration, [rand_out], n_overtones\n",
    "    )\n",
    "    rand_buffs[\"\".join(map(str, rand_out))] = combine_buffers([inbuff, rand_hbuff])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{get_audio_el(inbuff)}{inhitstr}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from bigram distribution'\n",
    "    + \"\".join([f\"</br>{get_audio_el(v)} {k}\" for k, v in buffs.items()])\n",
    "    + \"</br></br>Samples from uniform distribution\"\n",
    "    + \"\".join([f\"</br>{get_audio_el(v)} {k}\" for k, v in rand_buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4655d",
   "metadata": {},
   "source": [
    "## Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1e5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The goal is to maximize the likelihood of the data w.r.t. the model parameters\n",
    "# This is equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "# which is equivalent to minimizing the negative log likelihood\n",
    "# which is equivalent to minimizing the average negative log likelihood\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "log_likelihood = 0.0\n",
    "nixs = 0\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the hits\n",
    "    x = (x > 0).to(torch.int32)\n",
    "    y = (y > 0).to(torch.int32)\n",
    "    for h1, h2 in zip(x.flatten(), y.flatten()):\n",
    "        ix1 = h1.item()\n",
    "        ix2 = h2.item()\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        nixs += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/nixs}') # Average loss likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3c742",
   "metadata": {},
   "source": [
    "# Chroma\n",
    "\n",
    "## Creating chroma bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071006b",
   "metadata": {},
   "source": [
    "Specifically we use binary chromas, which are (N, 12) matrices of pitch class activations, where N is the number of timesteps and the values show the presence or absence of the pitch class.\n",
    "\n",
    "We could use binary pitch class strings of length 12 (e.g. `100100100100`) as tokens. That would give us a vocab size of 2**12 = 4096 (e.g. `perms = [''.join(map(str, i)) for i in itertools.product([0, 1], repeat=12)]` ). With the 36752 pairs in `babyslakh_20` that would give us a data : vocab ratio of ~9. Makemore has a 8500x data : vocab ratio (228146 pairs : vocab 27). To get a similar 8500x ratio we need ~35M pairs. Assuming the `babyslakh_20` average of ~1.8k pairs per file, we'd need ~19k files. `lmd_clean` is 17k files which is close, but it takes a long time to process and work with.\n",
    "\n",
    "We opt to instead shrink the vocab size by labeling the pitch class strings with [chord types](https://github.com/p3zo/chord-progressions/blob/main/chord_progressions/type_templates.py). That gives a vocab size of 193 (191 templates + 1 silence token + 1 out-of-vocab token). A vocab size of 193 wants ~1.7M pairs for the same ratio as makemore, which should be around 1k files. So we use `lmd_clean_1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e31850",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_4res\",\n",
    "    \"part_1\": 'Guitar',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"chroma\",\n",
    "    \"repr_2\": \"chroma\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "TYPE_TEMPLATES.update({'silence': '000000000000'})\n",
    "vocab_size = len(TYPE_TEMPLATES)\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "N = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
    "\n",
    "inv_type_templates = {v: k for k, v in TYPE_TEMPLATES.items()}\n",
    "stoi = {s:i for i,s in enumerate(inv_type_templates)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos[0] = '000000000000' # Use silence for out-of-vocab\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the chromas\n",
    "    x = (x > 1).to(torch.int32)\n",
    "    y = (y > 1).to(torch.int32)\n",
    "    for h1, h2 in zip(x, y):\n",
    "        for xrow, yrow in zip(h1, h2):\n",
    "            xtype = get_type_from_template(xrow.tolist())\n",
    "            ytype =  get_type_from_template(yrow.tolist())\n",
    "            x_template_str = TYPE_TEMPLATES[xtype]\n",
    "            y_template_str = TYPE_TEMPLATES[ytype]\n",
    "            ixx = stoi[x_template_str]\n",
    "            ixy = stoi[y_template_str]\n",
    "            N[ixx, ixy] += 1\n",
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "print(N.float().min(), N.float().max())\n",
    "print(P.min(), P.max())\n",
    "print(N.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4eef9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = f\"{dataset_config['part_1']}, {dataset_config['part_2']}\"\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(N, cmap='Blues');\n",
    "plt.title(title)\n",
    "plt.xticks(range(vocab_size), labels=list(TYPE_TEMPLATES), rotation=90, fontsize=5);\n",
    "plt.yticks(range(vocab_size), labels=list(TYPE_TEMPLATES), fontsize=5);\n",
    "for i in tqdm(range(vocab_size)):\n",
    "    for j in range(vocab_size):\n",
    "#         chstr = '_'.join([inv_type_templates[itos[i]], inv_type_templates[itos[j]]])\n",
    "        chstr = '|'.join([str(i), str(j)])\n",
    "        val = N[i, j].item()\n",
    "        if val > 1:\n",
    "            plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray', fontsize=6)\n",
    "            plt.text(j, i, round(val, 2), ha=\"center\", va=\"top\", color='gray', fontsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80a8ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataset_config['part_1'], dataset_config['part_2'])\n",
    "for ix, i in enumerate(TYPE_TEMPLATES):\n",
    "    v = N[ix].sum().item()\n",
    "    if v > 0:\n",
    "        print(v, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f11d4",
   "metadata": {},
   "source": [
    "## Sampling chroma\n",
    "\n",
    "Using chord types as our vocabulary has the added benefit of being able to use can use `chord_progressions.solver` to find voicings for templates that we sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173a607",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 3\n",
    "duration = 3  # seconds\n",
    "\n",
    "input_chord_types = [\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"unison\",\n",
    "    \"minor chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "]\n",
    "in_chroma = np.array([list(map(int, TYPE_TEMPLATES[i])) for i in input_chord_types])\n",
    "in_voiced_hits = get_voiced_hits_from_chroma(in_chroma)\n",
    "in_buff = mk_voiced_chroma_buffer(in_voiced_hits, duration=duration, n_overtones=1)\n",
    "in_hits = (in_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "\n",
    "buffs = collections.defaultdict(list)\n",
    "rand_buffs = collections.defaultdict(list)\n",
    "for i in range(n_samples):\n",
    "    rand_chroma = []\n",
    "    out_chroma = []\n",
    "    for t in in_chroma:\n",
    "        tix = stoi[TYPE_TEMPLATES[get_type_from_template(t)]]\n",
    "        p = P[tix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        pchromavec = list(map(int, itos[ix]))\n",
    "        out_chroma.append(pchromavec)\n",
    "\n",
    "        rand_p = torch.ones(vocab_size) / vocab_size  # uniform distribution\n",
    "        rand_ix = torch.multinomial(rand_p, num_samples=1, replacement=True, generator=g).item()\n",
    "        # For now, map out-of-vocabulary token to silence\n",
    "        if rand_ix == 0:\n",
    "            rand_ix = 192\n",
    "        rand_pchromavec = list(map(int, itos[rand_ix]))\n",
    "        rand_chroma.append(rand_pchromavec)\n",
    "\n",
    "    out_chroma = np.array(out_chroma)\n",
    "    out_hits = (out_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    out_voiced_hits = get_voiced_hits_from_chroma(out_chroma)\n",
    "    out_buff = mk_voiced_chroma_buffer(out_voiced_hits, duration=duration, n_overtones=4)\n",
    "    buffs[\"\".join(map(str, out_hits))].append([out_buff, combine_buffers([in_buff, out_buff])])\n",
    "\n",
    "    rand_chroma = np.array(rand_chroma)\n",
    "    rand_hits = (rand_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    rand_voiced_hits = get_voiced_hits_from_chroma(rand_chroma)\n",
    "    rand_buff = mk_voiced_chroma_buffer(rand_voiced_hits, duration=duration, n_overtones=4)\n",
    "    rand_buffs[\"\".join(map(str, rand_hits))].append([rand_buff, combine_buffers([in_buff, rand_buff])])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{\"\".join(map(str, in_hits))}</br>{get_audio_el(in_buff)}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from bigram distribution'\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in buffs.items()])\n",
    "    + \"</br></br>Samples from uniform distribution\"\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in rand_buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea261c22",
   "metadata": {},
   "source": [
    "## Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642909b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "nixs = 0\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the chromas\n",
    "    x = (x > 1).to(torch.int32)\n",
    "    y = (y > 1).to(torch.int32)\n",
    "    for h1, h2 in zip(x, y):\n",
    "        for xrow, yrow in zip(h1, h2):\n",
    "            xtype = get_type_from_template(xrow.tolist())\n",
    "            ytype =  get_type_from_template(yrow.tolist())\n",
    "            x_template_str = TYPE_TEMPLATES[xtype]\n",
    "            y_template_str = TYPE_TEMPLATES[ytype]\n",
    "            ixx = stoi[x_template_str]\n",
    "            ixy = stoi[y_template_str]\n",
    "\n",
    "            prob = P[ixx, ixy]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            nixs += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/nixs}') # Average loss likelihood\n",
    "# The lower this number is, the better the model is because it means it is assigning\n",
    "# high probabilities to the actual chord type pair in all the bigrams in our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea0760",
   "metadata": {},
   "source": [
    "# Recast the problem into a neural network framework\n",
    "\n",
    "Still using chroma bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of bigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the chromas\n",
    "    x = (x > 1).to(torch.int32)\n",
    "    y = (y > 1).to(torch.int32)\n",
    "    for h1, h2 in zip(x, y):\n",
    "        for xrow, yrow in zip(h1, h2):\n",
    "            xtype = get_type_from_template(xrow.tolist())\n",
    "            ytype =  get_type_from_template(yrow.tolist())\n",
    "            x_template_str = TYPE_TEMPLATES[xtype]\n",
    "            y_template_str = TYPE_TEMPLATES[ytype]\n",
    "            ixx = stoi[x_template_str]\n",
    "            ixy = stoi[y_template_str]\n",
    "            xs.append(ixx)\n",
    "            ys.append(ixy)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "n_examples = xs.nelement()\n",
    "print(f'Number of examples {n_examples}')\n",
    "\n",
    "# Initialize the \"network\" with a single linear layer of randomly initialized weights\n",
    "g = torch.Generator().manual_seed(15987348311)\n",
    "W = torch.randn((vocab_size, vocab_size), generator=g, requires_grad=True)\n",
    "\n",
    "# One-hot encode the input to the network\n",
    "xenc = F.one_hot(xs, num_classes=vocab_size).float()\n",
    "print(xenc.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edda71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1000):\n",
    "    # Forward pass\n",
    "    # Predict log-counts\n",
    "    logits = xenc @ W\n",
    "\n",
    "    # Softmax\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for each chord type\n",
    "\n",
    "    # Compute avg negative log likelihood\n",
    "    loss = -probs[torch.arange(n_examples), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if k % 100 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None # set the gradient to 0\n",
    "    loss.backward()\n",
    "\n",
    "    # W.grad has the same shape as W\n",
    "    # Each element of the gradient tells us the influence of that weight on the loss function\n",
    "    # Update the weights based on the gradient\n",
    "    W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[10][10], W.exp()[10][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(15987348311)\n",
    "\n",
    "n_samples = 3\n",
    "duration = 3  # seconds\n",
    "\n",
    "input_chord_types = [\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"unison\",\n",
    "    \"minor chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"major chord\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "    \"silence\",\n",
    "]\n",
    "in_chroma = np.array([list(map(int, TYPE_TEMPLATES[i])) for i in input_chord_types])\n",
    "in_voiced_hits = get_voiced_hits_from_chroma(in_chroma)\n",
    "in_buff = mk_voiced_chroma_buffer(in_voiced_hits, duration=duration, n_overtones=1)\n",
    "in_hits = (in_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "\n",
    "buffs = collections.defaultdict(list)\n",
    "rand_buffs = collections.defaultdict(list)\n",
    "for i in range(n_samples):\n",
    "    rand_chroma = []\n",
    "    out_chroma = []\n",
    "    for t in in_chroma:\n",
    "        tix = stoi[TYPE_TEMPLATES[get_type_from_template(t)]]\n",
    "        # ----\n",
    "        # BEFORE:\n",
    "        # p = P[tix]\n",
    "        # ----\n",
    "        # NOW\n",
    "        xenc = F.one_hot(torch.tensor([tix]), num_classes=vocab_size).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        # ----\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        pchromavec = list(map(int, itos[ix]))\n",
    "        out_chroma.append(pchromavec)\n",
    "\n",
    "    out_chroma = np.array(out_chroma)\n",
    "    out_hits = (out_chroma.sum(axis=1) > 0).astype(np.int8)\n",
    "    out_voiced_hits = get_voiced_hits_from_chroma(out_chroma)\n",
    "    out_buff = mk_voiced_chroma_buffer(out_voiced_hits, duration=duration, n_overtones=4)\n",
    "    buffs[\"\".join(map(str, out_hits))].append([out_buff, combine_buffers([in_buff, out_buff])])\n",
    "\n",
    "html = (\n",
    "    f'Input {dataset_config[\"part_1\"]}</br>{\"\".join(map(str, in_hits))}</br>{get_audio_el(in_buff)}'\n",
    "    + f'</br></br>Predicted {dataset_config[\"part_2\"]}</br></br>Samples from learned distribution'\n",
    "    + \"\".join([f\"</br>{k}</br>{get_audio_el(v[0][0])}predicted</br>{get_audio_el(v[0][1])}combined\" for k, v in buffs.items()])\n",
    ")\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2c95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf9cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
