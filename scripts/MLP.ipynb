{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13daa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from chord_progressions.type_templates import get_template_from_template_str, TYPE_TEMPLATES\n",
    "from chord_progressions.utils import is_circular_match\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rhythmic_relationships.data import PairDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c495cb",
   "metadata": {},
   "source": [
    "## Define the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8efbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 193\n"
     ]
    }
   ],
   "source": [
    "# Add silence\n",
    "silence_template = '000000000000'\n",
    "TYPE_TEMPLATES.update({'silence': silence_template})\n",
    "\n",
    "# Rename out-of-vocab token and use silence to fill it\n",
    "if '' in TYPE_TEMPLATES:\n",
    "    del TYPE_TEMPLATES['']\n",
    "TYPE_TEMPLATES['oov'] = silence_template\n",
    "\n",
    "vocab_size = len(TYPE_TEMPLATES)\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "inv_type_templates = {v: k for k, v in TYPE_TEMPLATES.items()}\n",
    "stoi = {s:i for i,s in enumerate(inv_type_templates)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itot = {ix:i for ix, i in enumerate(list(TYPE_TEMPLATES))}\n",
    "\n",
    "def get_type_from_template(template):\n",
    "    for chord_type in list(TYPE_TEMPLATES):\n",
    "        if is_circular_match(\n",
    "            template,\n",
    "            get_template_from_template_str(TYPE_TEMPLATES[chord_type]),\n",
    "        ):\n",
    "            return chord_type\n",
    "\n",
    "    return \"oov\" # out of vocabulary\n",
    "\n",
    "def pclist_to_i(pclist):\n",
    "    \"\"\"Gets a chord type index from a pitch class list\n",
    "    e.g. pclist_to_i([1,0,0,0,0,0,0,0,0,0,0,0]) -> 1\n",
    "    \"\"\"\n",
    "    chord_type = get_type_from_template(pclist)\n",
    "    template_str = TYPE_TEMPLATES[chord_type]\n",
    "    return stoi[template_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f401c73",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b74f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1412/1412 [00:19<00:00, 71.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_examples=67776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_config = {\n",
    "    \"dataset_name\": \"babyslakh_20_1bar_4res\",\n",
    "    \"part_1\": 'Guitar',\n",
    "    \"part_2\": 'Piano',\n",
    "    \"repr_1\": \"chroma\",\n",
    "    \"repr_2\": \"chroma\",\n",
    "}\n",
    "\n",
    "data = PairDataset(**dataset_config)\n",
    "loader = DataLoader(data, batch_size=1)\n",
    "\n",
    "# How many chords types do we use to predict the next one?\n",
    "# DO NOT CHANGE until the code for constructing the context is written more generally\n",
    "context_length = 3\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "silence_tensor = torch.tensor(list(map(int, silence_template))).reshape((1, 12)).float()\n",
    "\n",
    "for x, y in tqdm(loader):\n",
    "    # Binarize the chromas\n",
    "    x = (x > 1).to(torch.int32)[0]\n",
    "    y = (y > 1).to(torch.int32)[0]\n",
    "\n",
    "    # fill context with silence\n",
    "    context = [stoi[TYPE_TEMPLATES['silence']]] * context_length\n",
    "\n",
    "    for xrow, yrow in zip(x, y):\n",
    "        ixx = pclist_to_i(xrow.tolist())\n",
    "        ixy = pclist_to_i(yrow.tolist())\n",
    "        # print(','.join(itot[i] for i in context), '-->', itot[ixy])\n",
    "\n",
    "        X.append(context)\n",
    "        Y.append(ixy)\n",
    "\n",
    "        # Hard-code context to length 3 for now\n",
    "        # TODO: construct the context more generally\n",
    "        if len(Y[-2:]) < 2:\n",
    "            context = context[1:] = [Y[-1]] + [ixx]\n",
    "            continue\n",
    "        context = Y[-2:] + [ixx] # the previous 2 Ys and the current X\n",
    "\n",
    "        # Generally, I think we want to hold an even amount of each, but if the context has an odd length, hold an extra y.\n",
    "        #   e.g. for context length c\n",
    "        #     n_ys = c // 2 if c % 2 == 0 else c // 2 + 1\n",
    "        #     n_xs = c // 2\n",
    "\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n_examples = X.nelement()\n",
    "print(f'{n_examples=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29d94ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22592, 3]), torch.int64, torch.Size([22592]), torch.int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40772e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num total params: 20579\n"
     ]
    }
   ],
   "source": [
    "# Bengio et al has a vocab size of 17k and they embed them in a 30-dimensional space\n",
    "# In our case we have a vocab size of only 193, so we can use a much smaller space, even 2D\n",
    "embedding_dims = 2\n",
    "embedding_size = context_length * embedding_dims\n",
    "\n",
    "g = torch.Generator().manual_seed(73709238413)\n",
    "\n",
    "# Initialize an embeddings vector randomly\n",
    "C = torch.randn((vocab_size, embedding_dims), generator=g)\n",
    "\n",
    "num_neurons = 100\n",
    "W1 = torch.randn((embedding_size, num_neurons), generator=g)\n",
    "b1 = torch.randn(num_neurons, generator=g)\n",
    "W2 = torch.randn((num_neurons, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f'num total params: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ffbd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.9330, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, embedding_size) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "# These three commented lines are equal to the single F.cross_entropy line above\n",
    "# counts = logits.exp()\n",
    "# prob = counts / counts.sum(1, keepdims=True)\n",
    "# loss = -prob[torch.arange(X.shape[0]), Y].log().mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0819806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
